# -*- coding: utf-8 -*-
"""New_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15hc2ryV9G-fOk2gtcT6FsD-zNCM9xIAx
"""

!pip install python_speech_features

################################################################################

from google.colab import drive
drive.mount('/content/drive')

################################################################################

################################################################################

import os
import librosa
import librosa.display
import struct
import pandas as pd
import numpy as np
from datetime import datetime 
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.python.keras import backend
from keras.utils import to_categorical
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras import backend as K
import matplotlib.pyplot as plt
from keras.callbacks import ModelCheckpoint
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

################################################################################

################################################################################
# Creating Spectrograms of test images and then predicting results with the current model.

'''
n_fft = 2048
hop_length = 512
stft_all = np.array([])
path = '/content/drive/My Drive/Audio/train'
count = 0
for root, dir, files in os.walk(path):
  for file in files:

    # Loading the file by creating the directory
    file_path = root + '/' + file
    sample, sr = librosa.load(file_path, sr=10000)

    # Performing short term fourier transformation to the audio clip
    stft = librosa.core.spectrum.stft(sample, hop_length=hop_length, n_fft=n_fft)
    spectrogram = np.abs(stft)

    #Converting the spectrum to log specturm to clear out the spectogram images
    log_spectrogram = librosa.amplitude_to_db(spectrogram)
    librosa.display.specshow(log_spectrogram, sr = sr, hop_length = hop_length)

    
    plt.title(file)
    #plt.colorbar()
    filename, extension = os.path.splitext(file)
    #plt.savefig( '/content/drive/My Drive/Audio/train/log/' + filename + '.jpg')
    plt.show()
    count += 1
    print(count, end=" ") 
'''
################################################################################

################################################################################

## Running our model using ImageDataGenerator library on the test spectrogram images.
### Specifying the pixel images sizes, number of epochs, number of training and validation samples. 

from keras.preprocessing.image import ImageDataGenerator 
img_width, img_height = 432, 288
nb_train_samples = 270 
nb_validation_samples = 30
epochs = 50
batch_size = 32

from keras.preprocessing.image import ImageDataGenerator 

train_datagen = ImageDataGenerator( 
    rescale=1./ 255,  
    validation_split=0.1)
 
train_data_dir = '/content/drive/My Drive/Audio/train/log'

train_generator = train_datagen.flow_from_directory( 
    train_data_dir, 
    target_size=(img_width, img_height), 
    batch_size=batch_size, 
    classes = ['alert', 'clearthroat', 'cough', 'drawer', 'doorslam', 'keys', 'keyboard', 'knock', 'laughter', 'mouse', 'pageturn', 'pendrop',
               'phone', 'printer', 'speech', 'switch'],
    class_mode= 'categorical',
    subset = 'training')

validation_generator = train_datagen.flow_from_directory( 
    train_data_dir, 
    target_size=(img_width, img_height), 
    batch_size=batch_size, 
    classes = ['alert', 'clearthroat', 'cough', 'drawer', 'doorslam', 'keys', 'keyboard', 'knock', 'laughter', 'mouse', 'pageturn', 'pendrop',
               'phone', 'printer', 'speech', 'switch'],
    class_mode='categorical',
    subset = 'validation')

################################################################################

################################################################################
### Creating input shape for CNN, based on the condition if the image is grayscale or colored.

dir = '/content/drive/My Drive/Audio/train/Spectro'

if K.image_data_format() == 'channels_first': 
    input_shape = (3, img_width, img_height) 
else: 
    input_shape = (img_width, img_height, 3)
    
################################################################################

################################################################################
### Creating Sequential model to train our audio samples, using 2D Convulutional Network.

from tensorflow.keras import regularizers
model = Sequential() 
model.add(Conv2D(32, (2, 2), input_shape=input_shape)) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size=(2, 2))) 
  
model.add(Conv2D(32, (2, 2))) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size=(2, 2))) 

model.add(Conv2D(32, (2, 2))) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size=(2, 2)))
  
model.add(Conv2D(64, (2, 2))) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size=(2, 2))) 

model.add(Conv2D(64, (2, 2))) 
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size=(2, 2)))
  
model.add(Flatten()) 

model.add(Dense(units = 512, activation = 'relu')) 
model.add(Dropout(0.5))

model.add(Dense(units = 128, activation = 'relu')) 
model.add(Dropout(0.5))

model.add(Dense(units = 16, activation = 'softmax')) 
model.summary()

################################################################################

################################################################################
### Creating checkpoint to save the best weight with best accuracy.

from keras.callbacks import ModelCheckpoint
model.compile(loss='categorical_crossentropy', 
              optimizer='adam', 
              metrics=['categorical_accuracy'])

checkpoint = ModelCheckpoint('Best_modelweight.h5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

history = model.fit_generator( 
    train_generator, 
    steps_per_epoch=nb_train_samples // batch_size, 
    epochs= epochs, 
    validation_data=validation_generator, 
    validation_steps=nb_validation_samples,
    callbacks = callbacks_list)

model.save_weights('model_saved.h5')

################################################################################

################################################################################
## Plotting of Training Accuracy Vs Validation Accuracy on data points

plt.plot(history.history['categorical_accuracy'])
plt.plot(history.history['val_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='lower right')
plt.savefig('/content/drive/My Drive/Audio/train_accuracyVScat_accuracy.jpg')
plt.show()

################################################################################

################################################################################
## Plotting of Training Loss Vs Validation Loss on data points

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.savefig('/content/drive/My Drive/Audio/train_lossVScat_loss.jpg')
plt.show()

################################################################################

################################################################################
'''
import numpy as np
n_fft = 2048
hop_length = 512
stft_all = np.array([])
path = '/content/drive/My Drive/Audio/test/'
count = 0
for root, dir, files in os.walk(path):
  for file in files:

    # Loading the file by creating the directory
    file_path = root + '/' + file
    sample, sr = librosa.load(file_path, sr=22000)

    # Performing short term fourier transformation to the audio clip
    stft = librosa.core.spectrum.stft(sample, hop_length=hop_length, n_fft=n_fft)
    spectrogram = np.abs(stft)

    #Converting the spectrum to log specturm to clear out the spectogram images
    log_spectrogram = librosa.amplitude_to_db(spectrogram)
    librosa.display.specshow(log_spectrogram, sr = sr, hop_length = hop_length)

    
    plt.title(file)
    #plt.colorbar()
    filename, extension = os.path.splitext(file)
    plt.savefig( '/content/drive/My Drive/Audio/test/logarithm/log_sam/' + filename + '.jpg')
    plt.show()
    count += 1
    print(count, end=" ")
'''
################################################################################

################################################################################
### Steps to check accuracy of training data 
val_steps = 32 // batch_size
train_steps = 288 // batch_size

loss, categorical_accuracy = model.evaluate_generator(validation_generator, steps=val_steps)

print('Accuracy on validation data: {:.2f}%'.format(categorical_accuracy * 100))
print(loss)

################################################################################

################################################################################
### Creating test generator to predict test results
import numpy as np
import pandas as pd

test_dir = '/content/drive/My Drive/Audio/test/logarithm/'
test_datagen = ImageDataGenerator()
test_generator = test_datagen.flow_from_directory( 
    test_dir, 
    target_size=(img_width, img_height), 
    batch_size=1,
    classes = ['log_sam'],
    class_mode=None)

################################################################################

################################################################################
### Predicting test results using the above created test generator and exporting the results in a csv file

test_steps = test_generator.n // test_generator.batch_size

test_generator.reset()
predict = model.predict_generator(test_generator, steps=test_steps, verbose=1)

predicted_class_indices = np.argmax(predict, axis=1)
labels = (train_generator.class_indices)
labels = dict((v,k) for k,v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

filenames=test_generator.filenames
#print(filenames)
results=pd.DataFrame({"Filename":filenames,
                      "Predictions":predictions})
results.to_csv('/content/drive/My Drive/Audio/Results.csv')
print(results)

################################################################################